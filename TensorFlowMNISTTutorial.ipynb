{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow MNIST Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://www.tensorflow.org) is a framework created by Google meant to make programming various machine learning tasks more simple and easily distributable, with an emphasis on deep learning and neural networks. Other frameworks with a similar aim include [Theano](http://deeplearning.net/software/theano/) and [Torch](http://torch.ch).\n",
    "\n",
    "TensorFlow can be installed through a variety of resources from the command line. The tutorial at [this address](https://www.tensorflow.org/get_started/os_setup#pip_installation) will help walk through installation. Note that installation with GPU support is a separate package.\n",
    "\n",
    "Check that TensorFlow has been properly installed by performing the following import in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Philosophy Behind TensorFlow\n",
    "\n",
    "The most central idea in TensorFlow's organization is that machine learning updates occur across a graph that is defined by the user, and that actions that affect the graph are defined separately. In the same way one has to build a fountain before running water through it, the graph must be defined before we run computations across it.\n",
    "\n",
    "For example, the following code defines a node in a graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell, we haven't given this node a value. It is a placeholder that we will provide when we wish to run a computation using the graph. We must specify that when we provide an input to this node, it will be in the format of a 32-bit floating-point number. \n",
    "\n",
    "The reason that TensorFlow does this is that running many computations in Python can be slow and unweildly, so TensorFlow farms this code out to its optimized backend, which is written in C/C++. By defining the graph outside of Python, we can use Python as an interface for a much faster backend.\n",
    "\n",
    "We can obtain additional information using the print command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_2:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also has some negative connotations that make simple operations more complicated. For example, the following two code blocks perform the same operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 5.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"model\") as scope:\n",
    "    x = tf.placeholder(tf.float32)\n",
    "    y = tf.placeholder(tf.float32)\n",
    "    add_xy = tf.add(x, y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run([add_xy], feed_dict={x: [2.], y: [3.]})\n",
    "    print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, TensorFlow adds significant overhead here. However, the structure of this code will be essentially the same no matter what operation we are attempting to perform with this package.\n",
    "\n",
    "In the first block we define two placeholders, x and y, which we then use to perform the addition operation with the tf.add() function. This is our graph structure.\n",
    "\n",
    "In the second block we start a session and run the computations described by the graph relationship we set up previously, specifically asking for results to return the value of the add_xy variable, and passing in our placeholders' values as x = 2 and y = 3.\n",
    "\n",
    "The result value is the same type as the add_xy value after the computation has been run.\n",
    "\n",
    "# Why on earth do we do this?\n",
    "\n",
    "So why use this complicated formulation when a simple formula would be just as effective? The answer is because this complicated framework lends itself to a major feature of machine learning specification, and specifically neural networks: updates and batches.\n",
    "\n",
    "Once we have defined the relationships between our values, it is easy to repeat computations a number of times in order to train a model. It is also easy to introduce stochastic elements to the training process, like randomly censoring input nodes or values (known as dropout). \n",
    "\n",
    "In order to really understand the way we would do this, we must introduce the idea of `variables`. For a \"simple\" example, we can consider raising a number to a power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('model') as scope:\n",
    "  # initialize our variable at 1\n",
    "  x = tf.constant(2)\n",
    "  y = tf.Variable(1)\n",
    "  # define an operation on the graph\n",
    "  new_value = tf.mul(x,y)\n",
    "  update = tf.assign(y, new_value)\n",
    "  # initialize values\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(init)\n",
    "  print(session.run(y))\n",
    "  for i in range(10):\n",
    "    session.run(update)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example introduces a couple of new ideas to the mix. We define `y` to be a `Variable`. This allows this value to be updated over successive iterations over the graph. We use the `update` method of TensorFlow to change the value of `y` each time we run through the graph, and then dump its value using the `session.run` function, which allows us to retrieve the values stored for the current run of the graph.\n",
    "\n",
    "# Factorial\n",
    "\n",
    "We can extend the above example using a placeholder value in order to provide the value of a factorial. In this case we supply a list of values, which TensorFlow iterates over in the background. Each new value is multiplied by the current value of y and updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "6\n",
      "24\n",
      "120\n",
      "720\n",
      "5040\n",
      "40320\n",
      "362880\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('model') as scope:\n",
    "  # initialize our variable at 1\n",
    "  x = tf.placeholder(tf.int32)\n",
    "  y = tf.Variable(1)\n",
    "  # define an operation on the graph\n",
    "  new_value = tf.mul(x,y)\n",
    "  update = tf.assign(y, new_value)\n",
    "  # initialize values\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run(init)\n",
    "  for i in range(1,10):\n",
    "    session.run(update, feed_dict={x: i})\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables are used in machine learning applications for a variety of components that are updated behind the scenes in models. Things like weights and biases can be defined succinctly and manipulated with less mess than writing code from scratch. Placeholders generally serve to mark the nodes across which training and test data will be loaded. Instead of a series of numbers, this may represent a series of image data or other data streams. This organization allows the behaviors of these components to be specified in a more modular fashion, without needing to rewrite a framework from scratch every time a tweak is applied.\n",
    "\n",
    "# Training a Basic Neural Network\n",
    "\n",
    "While the former examples are much simpler than a machine learning model, they describe the major operations that TensorFlow performs in order to execute its model. We define a graph and the relationships that link its nodes, then we load data across that graph in order to modify it and arrive at a solution.\n",
    "\n",
    "We can extend this example into a full classifier that trains a neural network on the MNIST handwritten digit data set and can classify examples with decent results. This section of the tutorial will focus on a very slightly modified version of code made available on [TensorFlow's official page](https://www.tensorflow.org/tutorials/mnist/beginners/) (full code without tutorial available [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py)), and will attempt to explain the choices being made in the code in order to simplify its interpretation. We will not cover the specifics of the modeling choices made by the code, for example the differences between a softmax and ReLU function. Instead the focus will be on the implementation and organization of the code included in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These first imports get us updated handling of basic functions, relating to file handling, floating point division, and print function syntax, respectively. We also import the MNIST data set and TensorFlow itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code imports and extracts the MNIST data set from its location within the TensorFlow tutorial section. The `one_hot` flag tells TensorFlow that we would like this data arranged in [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). As we are dealing with digits 0-9, we are concerned with ten classes. For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the number 3 in one-hot encoding\n",
    "[0,0,0,1,0,0,0,0,0,0]\n",
    "\n",
    "# An example of the number 7 in one-hot encoding\n",
    "[0,0,0,0,0,0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are tasked with defining the model we will apply to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to define a lot of information in these four lines, and ultimately this is the relationship that makes the neural network perform its job. The basic formulation is as follows:\n",
    "\n",
    "`y = x*W + b`\n",
    "\n",
    "...where x is a vector of information that we will be training on, W is the weight values of the edges of the neural network, and b is the bias that is applied at each output node.\n",
    "\n",
    "Our images are arranged in a 28x28 grid, giving us 784 pixels to train on. Each pixel will be an input to our neural net. The weights and biases are set to zero, but as TensorFlow variables they have the ability to be modified as the model is trained.\n",
    "\n",
    "Next we must define what actions we will apply to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up code to compare our actual values (`y_`) to the predicted values (`y`). We define a cross entropy function, which gives us the loss of the model or how far our predictions are off from the actual values. Essentially, it compares for each class how much likelihood the classifier gave that the example fell in that group, and how close it was to the actual chance of it being in that group. (More information available [here](http://colah.github.io/posts/2015-09-Visual-Information/))\n",
    "\n",
    "Next we set up an operation called `train_step`, which uses the results of the `cross_entropy` calculation to adjust weights across the graph. Here we define that we want to use a gradient descent algorithm (one of many options given by TensorFlow) and we set it to have a learning rate of 0.5, asking it to minimize the `cross_entropy` value.\n",
    "\n",
    "*It is worth pausing here for a note.* The above block of code is one of the most magical in the TensorFlow tutorial. It very concisely sets up the backpropagation procedure in a very small space. TensorFlow is able to infer from the structure of the graph what a backpropagation cycle constitutes and to implement it given very little information. This is the real power that TensorFlow affords. By specifying our model in simple terms, we can build very complex learning systems that are very powerful.\n",
    "\n",
    "Before we can run the actual model, we must perform some housekeeping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define that we will be running our code in an interactive session in the interpreter. We then initialize the variables that are encoded in our graph with the values that are specified for their setup. We are now ready to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify that we will be selecting batches of 100 digits from the training set, divided into inputs (`batch_xs`) and true labels (`batch_ys`). We compute one step of gradient descent in the model, based on our results from those 100 digits. This process is repeated 1000 times.\n",
    "\n",
    "This simple process is very powerful and can be performed for many different kinds of learning models. The process of running random batches across the graph may seem less than ideal in practice, but is very powerful when training data are sufficiently large.\n",
    "\n",
    "We can now go to our test set and see how well our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9172\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                    y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code is very confusing, but is actually just a very pithy way of performing some basic mathematical calculations. We define a correct prediction to be a case where the correct classification was also the highest predicted value for a class. We then take the mean of the correctly predicted values (a list of zeros and ones) which gives us the accuracy value. We direct the model to go into our session and extract this computation from the graph, provided the test dataset.\n",
    "\n",
    "The resulting value should be between 90% and 93% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "The above model is a fully specified neural network with error checking. Without imports it is encapsulated in only *16 lines of code.* The model is also highly extensible, and many variables can be changed by changing a single line of code. While other packages exist to define a similar model with less hassle (for example in R or the scikit-learn module of python) the amount of flexibility and optimization is much higher with TensorFlow.\n",
    "\n",
    "TensorFlow is much more than a toolkit for running machine learning. It is a flexible *modeling framework* that allows many different models to be created and extended by thinking in the context of a graph-based system. Many different learning models can be implemented given that you have a firm grasp on the structure and supplied tools.\n",
    "\n",
    "# Additional Exercises\n",
    "\n",
    "In order to get a handle on how this basic example can be updated in order to make it a multilayer neural network (a \"deep learning\" convolutional neural network model) take a look at the code provided in [this TensorFlow tutorial](https://www.tensorflow.org/tutorials/mnist/pros/).\n",
    "\n",
    "Many other examples are available at the [TensorFlow tutorial page](https://www.tensorflow.org/tutorials/) with much deeper explanations of the mathematic considerations for building models, especially neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
